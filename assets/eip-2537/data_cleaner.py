
import numpy as np
import pandas as pd
from scipy import stats

class DataCleaner:
    def __init__(self, df):
        self.df = df.copy()
        self.original_columns = df.columns.tolist()
    
    def remove_outliers_zscore(self, columns=None, threshold=3):
        if columns is None:
            columns = self.df.select_dtypes(include=[np.number]).columns
        
        df_clean = self.df.copy()
        for col in columns:
            if col in df_clean.columns:
                z_scores = np.abs(stats.zscore(df_clean[col].dropna()))
                mask = z_scores < threshold
                df_clean = df_clean[mask | df_clean[col].isna()]
        
        self.df = df_clean.reset_index(drop=True)
        return self
    
    def normalize_minmax(self, columns=None):
        if columns is None:
            columns = self.df.select_dtypes(include=[np.number]).columns
        
        for col in columns:
            if col in self.df.columns:
                col_min = self.df[col].min()
                col_max = self.df[col].max()
                if col_max > col_min:
                    self.df[col] = (self.df[col] - col_min) / (col_max - col_min)
        
        return self
    
    def fill_missing_median(self, columns=None):
        if columns is None:
            columns = self.df.select_dtypes(include=[np.number]).columns
        
        for col in columns:
            if col in self.df.columns:
                median_val = self.df[col].median()
                self.df[col] = self.df[col].fillna(median_val)
        
        return self
    
    def get_cleaned_data(self):
        return self.df
    
    def get_summary(self):
        summary = {
            'original_rows': len(self.df),
            'cleaned_rows': len(self.df),
            'numeric_columns': self.df.select_dtypes(include=[np.number]).columns.tolist(),
            'categorical_columns': self.df.select_dtypes(exclude=[np.number]).columns.tolist()
        }
        return summary

def clean_dataset(df, outlier_threshold=3, normalize=True, fill_missing=True):
    cleaner = DataCleaner(df)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    if len(numeric_cols) > 0:
        cleaner.remove_outliers_zscore(threshold=outlier_threshold)
        
        if fill_missing:
            cleaner.fill_missing_median()
        
        if normalize:
            cleaner.normalize_minmax()
    
    return cleaner.get_cleaned_data(), cleaner.get_summary()
import pandas as pd
import numpy as np

def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

def clean_dataset(input_path, output_path):
    df = pd.read_csv(input_path)
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_columns:
        original_count = len(df)
        df = remove_outliers_iqr(df, col)
        removed_count = original_count - len(df)
        print(f"Removed {removed_count} outliers from column: {col}")
    
    df.to_csv(output_path, index=False)
    print(f"Cleaned data saved to: {output_path}")
    return df

if __name__ == "__main__":
    input_file = "raw_data.csv"
    output_file = "cleaned_data.csv"
    cleaned_df = clean_dataset(input_file, output_file)import numpy as np
import pandas as pd
from scipy import stats

def detect_outliers_iqr(data, column, threshold=1.5):
    """
    Detect outliers using IQR method
    """
    q1 = data[column].quantile(0.25)
    q3 = data[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - threshold * iqr
    upper_bound = q3 + threshold * iqr
    
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers

def remove_outliers_zscore(data, column, threshold=3):
    """
    Remove outliers using Z-score method
    """
    z_scores = np.abs(stats.zscore(data[column]))
    filtered_data = data[z_scores < threshold]
    return filtered_data

def normalize_minmax(data, column):
    """
    Normalize data using Min-Max scaling
    """
    min_val = data[column].min()
    max_val = data[column].max()
    data[column + '_normalized'] = (data[column] - min_val) / (max_val - min_val)
    return data

def standardize_data(data, column):
    """
    Standardize data using Z-score normalization
    """
    mean_val = data[column].mean()
    std_val = data[column].std()
    data[column + '_standardized'] = (data[column] - mean_val) / std_val
    return data

def handle_missing_values(data, strategy='mean'):
    """
    Handle missing values with specified strategy
    """
    if strategy == 'mean':
        return data.fillna(data.mean())
    elif strategy == 'median':
        return data.fillna(data.median())
    elif strategy == 'mode':
        return data.fillna(data.mode().iloc[0])
    elif strategy == 'drop':
        return data.dropna()
    else:
        raise ValueError("Invalid strategy. Choose from 'mean', 'median', 'mode', or 'drop'")

def clean_dataset(data, numeric_columns, outlier_method='zscore', normalization_method='standardize'):
    """
    Main function to clean dataset with multiple steps
    """
    cleaned_data = data.copy()
    
    # Handle missing values
    for col in numeric_columns:
        cleaned_data = handle_missing_values(cleaned_data, strategy='mean')
    
    # Remove outliers
    if outlier_method == 'zscore':
        for col in numeric_columns:
            cleaned_data = remove_outliers_zscore(cleaned_data, col)
    elif outlier_method == 'iqr':
        for col in numeric_columns:
            outliers = detect_outliers_iqr(cleaned_data, col)
            cleaned_data = cleaned_data.drop(outliers.index)
    
    # Apply normalization
    if normalization_method == 'standardize':
        for col in numeric_columns:
            cleaned_data = standardize_data(cleaned_data, col)
    elif normalization_method == 'minmax':
        for col in numeric_columns:
            cleaned_data = normalize_minmax(cleaned_data, col)
    
    return cleaned_data

def validate_data(data, numeric_columns):
    """
    Validate cleaned data for common issues
    """
    validation_report = {}
    
    for col in numeric_columns:
        validation_report[col] = {
            'has_nulls': data[col].isnull().any(),
            'null_count': data[col].isnull().sum(),
            'mean': data[col].mean(),
            'std': data[col].std(),
            'min': data[col].min(),
            'max': data[col].max()
        }
    
    return pd.DataFrame(validation_report).T

# Example usage
if __name__ == "__main__":
    # Create sample data
    np.random.seed(42)
    sample_data = pd.DataFrame({
        'feature1': np.random.normal(100, 15, 1000),
        'feature2': np.random.exponential(50, 1000),
        'feature3': np.random.uniform(0, 200, 1000)
    })
    
    # Add some outliers and missing values
    sample_data.iloc[10:15, 0] = np.nan
    sample_data.iloc[20:25, 1] = 500  # Outliers
    
    numeric_cols = ['feature1', 'feature2', 'feature3']
    
    # Clean the data
    cleaned = clean_dataset(sample_data, numeric_cols)
    
    # Validate results
    validation = validate_data(cleaned, numeric_cols)
    print("Data validation report:")
    print(validation)