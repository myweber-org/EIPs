import numpy as np
import pandas as pd
from scipy import stats

def remove_outliers_iqr(data, column, factor=1.5):
    """
    Remove outliers using IQR method
    """
    q1 = data[column].quantile(0.25)
    q3 = data[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - factor * iqr
    upper_bound = q3 + factor * iqr
    
    filtered_data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
    return filtered_data

def z_score_normalize(data, column):
    """
    Normalize data using z-score method
    """
    mean = data[column].mean()
    std = data[column].std()
    
    if std == 0:
        return data[column]
    
    normalized = (data[column] - mean) / std
    return normalized

def min_max_normalize(data, column):
    """
    Normalize data using min-max scaling
    """
    min_val = data[column].min()
    max_val = data[column].max()
    
    if max_val == min_val:
        return data[column]
    
    normalized = (data[column] - min_val) / (max_val - min_val)
    return normalized

def clean_dataset(df, numeric_columns, outlier_method='iqr', normalize_method='zscore'):
    """
    Main cleaning function for datasets
    """
    cleaned_df = df.copy()
    
    for col in numeric_columns:
        if col not in cleaned_df.columns:
            continue
            
        if outlier_method == 'iqr':
            cleaned_df = remove_outliers_iqr(cleaned_df, col)
        elif outlier_method == 'zscore':
            z_scores = np.abs(stats.zscore(cleaned_df[col]))
            cleaned_df = cleaned_df[z_scores < 3]
        
        if normalize_method == 'zscore':
            cleaned_df[f'{col}_normalized'] = z_score_normalize(cleaned_df, col)
        elif normalize_method == 'minmax':
            cleaned_df[f'{col}_normalized'] = min_max_normalize(cleaned_df, col)
    
    return cleaned_df

def validate_data(df, required_columns, min_rows=10):
    """
    Validate dataset structure and content
    """
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        raise ValueError(f"Missing required columns: {missing_columns}")
    
    if len(df) < min_rows:
        raise ValueError(f"Dataset has fewer than {min_rows} rows")
    
    return Trueimport csv
import re

def clean_csv(input_file, output_file):
    """
    Clean a CSV file by removing rows with missing values
    and standardizing text fields.
    """
    cleaned_rows = []
    
    with open(input_file, 'r', newline='', encoding='utf-8') as infile:
        reader = csv.DictReader(infile)
        fieldnames = reader.fieldnames
        
        for row in reader:
            # Skip rows with any empty values
            if any(value.strip() == '' for value in row.values()):
                continue
            
            # Clean text fields
            cleaned_row = {}
            for key, value in row.items():
                cleaned_value = value.strip()
                # Remove extra whitespace
                cleaned_value = re.sub(r'\s+', ' ', cleaned_value)
                # Capitalize first letter of each word for name fields
                if 'name' in key.lower():
                    cleaned_value = cleaned_value.title()
                cleaned_row[key] = cleaned_value
            
            cleaned_rows.append(cleaned_row)
    
    # Write cleaned data to new file
    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(cleaned_rows)
    
    return len(cleaned_rows)

def validate_email(email):
    """
    Validate email format using regex.
    """
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return bool(re.match(pattern, email))

def remove_duplicates(input_file, output_file, key_field):
    """
    Remove duplicate rows based on a key field.
    """
    unique_records = {}
    
    with open(input_file, 'r', newline='', encoding='utf-8') as infile:
        reader = csv.DictReader(infile)
        
        for row in reader:
            key = row[key_field]
            unique_records[key] = row
    
    # Write unique records to new file
    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:
        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)
        writer.writeheader()
        writer.writerows(unique_records.values())
    
    return len(unique_records)

if __name__ == "__main__":
    # Example usage
    input_csv = "raw_data.csv"
    output_csv = "cleaned_data.csv"
    
    try:
        rows_processed = clean_csv(input_csv, output_csv)
        print(f"Cleaned {rows_processed} rows from {input_csv}")
        print(f"Output saved to {output_csv}")
    except FileNotFoundError:
        print(f"Error: File {input_csv} not found")
    except Exception as e:
        print(f"Error processing file: {str(e)}")